<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Mind the Gap">
  <meta property="og:title" content="Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction"/>
  <meta property="og:description" content="Mind the Gap IJCB 2025"/>
  <meta property="og:url" content="YOUR_PROJECT_URL"/>
  <meta property="og:image" content="static/images/your_banner_image.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction">
  <meta name="twitter:description" content="Mind the Gap IJCB 2025">
  <meta name="twitter:image" content="static/images/your_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Gait Recognition, Occlusion, Deep Learning, Biometrics, Residual Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mind the Gap IJCB 2025</title>
  <link rel="icon" type="image/x-icon" href="static/images/your_icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ayush-00.github.io/" target="_blank">Ayush Gupta</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Siyuan Huang</a>,</span>
                <span class="author-block">
                    <a href="https://engineering.jhu.edu/faculty/rama-chellappa/" target="_blank">Rama Chellappa</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Johns Hopkins University<br>IJCB 2025</span>
                    </div>

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links">
                         <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="#" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <span class="link-block">
                    <a href="https://github.com/Ayush-00/rg-gait" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.10978" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Gait is becoming popular as a method of person re-identification because of its ability to identify people at a distance.  However, most current works in gait recognition do not address the practical problem of occlusions.  Among those which do, some require paired tuples of occluded and holistic sequences, which are impractical to collect in the real world.  Further, these approaches work on occlusions but fail to retain performance on holistic inputs.  To address these challenges, we propose RG-Gait, a method for residual correction for occluded gait recognition with holistic retention.  We model the problem as a residual learning task, conceptualizing the occluded gait signature as a residual deviation from the holistic gait representation.  Our proposed network adaptively integrates the learned residual, significantly improving performance on occluded gait sequences without compromising the holistic recognition accuracy.  We evaluate our approach on the challenging Gait3D, GREW and BRIAR datasets and show that learning the residual can be an effective technique to tackle occluded gait recognition with holistic retention. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item has-text-centered">
          <img src="static/images/briar-examples.png" alt="RGB examples" style="width: 60%; display: block; margin-left: auto; margin-right: auto;"/>
          <h2 class="subtitle has-text-centered">
            Sample RGB images from the BRIAR dataset, showing challenging real-world conditions.
          </h2>
        </div>
        <div class="item has-text-centered">
          <img src="static/images/synthetic_occlusions.png" alt="synthetic occlusions" />
          <h2 class="subtitle has-text-centered">
            Examples of synthetic occlusions used in our work.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
</div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Overview</h2>
        <figure class="image">
          <img src="static/images/framework.png" alt="main methodology figure">
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        
        <p class="has-text-justified">
           The core challenge in occluded gait recognition is the absence of complete information about a subject's walking pattern.  Our method, RG-Gait, addresses this through a three-module architecture that dynamically adapts to different levels of occlusion.  RG-Gait consists of three key components: (1) an Occlusion Evaluation Module (OEM) for detecting and quantifying occlusions, (2) a Feature Restoration Network (FRN) for compensating missing information, and (3) a Gait Signature Extractor (GSE) for capturing identity-specific gait features.  We model the occluded gait features as a residual deviation from the holistic features.  The FRN learns this residual, and it is adaptively integrated into the final gait signature based on the occlusion level detected by the OEM. This allows the model to rely on direct gait features when silhouettes are complete, while incorporating more restorative residual features when occlusions are significant. 
        </p>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Synthetic Occlusions</h2>
        <figure class="image">
          <img src="static/images/synthetic_occlusions.png" alt="Synthetic occlusions" style="width: 40%; display: block; margin-left: auto; margin-right: auto;">
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        
        <p class="has-text-justified">
            Following previous works, we experiment with synthetic occlusions of different types.  Specifically, we randomly occlude the top, bottom, or middle part of the subject in the input silhouette.  The size and position of the occlusion are chosen randomly during training and testing.  We also introduce a moving occlusion, where a black mask of varying size moves laterally across the video at different speeds, occluding different parts of the subject within the same sequence.  We follow settings where up to 60% of the subject may be occluded.  Importantly, we introduce these occlusions randomly, without the need for paired (occluded, complete) data during training. 
        </p>
      </div>
    </div>
  </div>
</section>
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Evaluation</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item has-text-centered">
          <img src="static/images/rank-retrieval.png" alt="Rank-Retrieval"/>
          <h2 class="subtitle has-text-centered">
            Rank-Retrieval and Holistic Performance Retention.
          </h2>
        </div>
        <div class="item has-text-centered">
          <img src="static/images/rp-visualize.png" alt="RP metric"/>
          <h2 class="subtitle has-text-centered">
            Relative Performance (RP) Metric.
          </h2>
        </div>
        <div class="item has-text-centered">
          <img src="static/images/generalize-adapt.png" alt="Generalizability and Adaptability"/>
          <h2 class="subtitle has-text-centered">
            Generalizability and Adaptability tests on new occlusion types.
          </h2>
        </div>
      </div>
      <div class="content has-text-justified">
        <p>
          We evaluate our method using multiple metrics. Rank-Retrieval accuracy (Rank-1, Rank-5) measures how well an unseen probe sample matches an identity in a gallery. To address the issue that occlusion training can reduce performance on non-occluded (holistic) data, we introduce the Holistic Performance Retention (HPR) evaluation, which measures how much performance is retained on complete data after occlusion training. We also use the Relative Performance (RP) metric, which evaluates a model's accuracy on occluded data relative to the upper-bound performance on holistic data, providing a standardized measure of how effectively a method handles occlusions. 
        </p>
        <p>
          Furthermore, we evaluate the generalizability and adaptability of our approach by testing on new, unseen occlusion types.  Generalizability refers to a model's ability to handle new occlusions without any retraining, while adaptability measures how well a model can perform on new occlusions after being fine-tuned on them. 
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Results</h2>
        <figure class="image">
            <img src="static/images/main-results-ijcb.png" alt="Main Results Table">
            <h2 class="subtitle has-text-centered">
              RG-Gait outperforms other methods on occluded gait recognition across multiple datasets and backbones.
            </h2>
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
           Our main results show that RG-Gait consistently performs better than other baseline methods on the occluded gait recognition task. This holds true across challenging datasets like Gait3D, GREW, and BRIAR, and when using different model backbones such as GaitBase and DeepGaitV2. We also tested our method with a transformer-based backbone, SwinGait, and observed that our approach was still more effective than the baselines. These results demonstrate that learning a corrective residual feature is an effective strategy for occluded gait recognition.
        </p>
      </div>
    </div>

    <div class="columns is-centered" style="margin-top: 2rem;">
        <div class="column is-full">
          <h3 class="title is-4 has-text-centered">Holistic Performance Retention</h3>
          <figure class="image">
              <img src="static/images/Holistic_performance_retainment.png" alt="Holistic Performance Retention" style="width: 60%; display: block; margin-left: auto; margin-right: auto;">
          </figure>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-full">
          <p class="has-text-justified">
            A key goal of our work was to handle occlusions without sacrificing performance on complete, non-occluded (holistic) data. When we evaluate models trained for occlusion on holistic data, we observe that other methods experience a significant performance drop compared to the original backbone's capability. In contrast, our method, RG-Gait, is specifically designed to retain most of the holistic performance of the original backbone, even after being trained on occluded data.
          </p>
        </div>
      </div>

  
  <div class="columns is-centered" style="margin-top: 2rem;">
      <div class="column is-full">
        <h3 class="title is-4 has-text-centered">Generalizability and Adaptability</h3>
        <figure class="image">
            <img src="static/images/gen-adapt-results-ijcb.png" alt="Generalizability and Adaptability Results" style="width: 60%; display: block; margin-left: auto; margin-right: auto;">
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
            For a gait recognition method to be practical, it must handle diverse occlusion types, but it is not feasible to train a model on all possible variations. We therefore test our model on new kinds of occlusions it has not seen during training. <strong>Generalizability</strong> refers to how well a model performs on new occlusions in a zero-shot scenario, without any retraining. <strong>Adaptability</strong> refers to the model's performance on new occlusions after it has been fine-tuned on them. Our experiments, which involve training on top and bottom occlusions and evaluating on middle and moving occlusions, show that RG-Gait can generalize and adapt to new occlusion types better than other methods.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3 has-text-centered">Ablations</h2>
        <figure class="image">
            <img src="static/images/Ablations-ijcb.png" alt="Ablations" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        </figure>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full">
        <p class="has-text-justified">
            We perform ablation studies to analyze the effect of the different components in our framework: occlusion residual learning, adaptive feature integration, and occlusion awareness.  Our experiments show that the Feature Restoration Network (FRN), which is responsible for learning the residual, is essential for performance on occluded data. Furthermore, adaptively integrating the residual features based on the occlusion score from the OEM improves results, indicating that the learned residual is more important in some scenarios than others.  Finally, providing occlusion awareness to the FRN in the form of a feature from the OEM further improves the quality of the generated residual. We find that all components complement each other to boost overall performance when used together. 
        </p>
      </div>
    </div>
  </div>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/K2SZy7R2syw?si=7CrNi4uA31R22Oyj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/RG-Gait poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{gupta2025ijcb,
          title={Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction},
          author={Ayush Gupta and Siyuan Huang and Rama Chellappa},
          booktitle={2025 IEEE International Joint Conference on Biometrics (IJCB)},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
  </html>